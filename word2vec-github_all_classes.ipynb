{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Содержание:\n",
    "\n",
    "    1) Метод word2vec с классификаторами:\n",
    "            1) Ridge Classifier\n",
    "            2) Perceptron\n",
    "            3) Passive Agressive\n",
    "            4) BeurnolliNB\n",
    "            5) MultinomialNB\n",
    "            6) KNeighbors\n",
    "            7) RandomForest\n",
    "            8) NearestCentroid\n",
    "            9) SGDClassifier\n",
    "            10) LinearSVC\n",
    "    2) Визуализация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pymorphy2\n",
    "import gensim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Загружаю данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sphere_6 = defaultdict(int)\n",
    "genre_fi_7 = defaultdict(int)\n",
    "type_8 = defaultdict(int)\n",
    "topic_9 = defaultdict(int)\n",
    "\n",
    "i = 0\n",
    "f = open('./out.csv', 'r')\n",
    "for line in f:\n",
    "    i += 1\n",
    "    attrs = line.split(';')\n",
    "    sphere_6[attrs[6]] += 1\n",
    "    genre_fi_7[attrs[7]] += 1\n",
    "    type_8[attrs[8]] += 1\n",
    "    topic_9[attrs[9]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# type_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('./out.csv', 'r')\n",
    "file_hash_all = {}\n",
    "for line in f:\n",
    "    attrs = line.split(';')\n",
    "    path = attrs[0]\n",
    "    class_name = attrs[8]\n",
    "    if ' ' in class_name:\n",
    "        class_name = class_name.split()[0]\n",
    "    if '|' in class_name:\n",
    "        class_name = class_name.split('|')[0]\n",
    "    file_name = os.path.basename(path)\n",
    "    dir_name = os.path.dirname(path)\n",
    "    full_dir = os.path.join('/Users/Valeriya/Desktop/CoursePaper/corpus', dir_name)\n",
    "    files = os.listdir(full_dir)\n",
    "    if file_name in files:\n",
    "        file_hash_all[file_name] = class_name\n",
    "    elif file_name + '.xml' in files:\n",
    "        file_hash_all[file_name + '.xml'] = class_name\n",
    "    elif file_name + '.xhtml' in files:\n",
    "        file_hash_all[file_name + '.xhtml'] = class_name\n",
    "    else:\n",
    "        edit = 1000\n",
    "        true_name = ''\n",
    "        for file_in_dir in files:\n",
    "            dist = distance(file_name, file_in_dir)\n",
    "            if dist < edit:\n",
    "                edit = dist\n",
    "                true_name = file_in_dir\n",
    "        file_hash_all[true_name] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14382"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_hash_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('./out.csv', 'r')\n",
    "cl = {'отчет':0,'поздравление':0, 'интервью':0, 'репортаж':0, 'хроника':0, 'объявление':0, 'комментарий':0,\n",
    "      'совет':0, 'анонс':0, 'заметка':0, 'информационное сообщение':0, 'статья':0}\n",
    "file_hash = {}\n",
    "for file_name, class_name in file_hash_all.items():\n",
    "    if class_name in cl:\n",
    "        cl[class_name] +=1\n",
    "        file_hash[file_name] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9847"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-19 09:04:59,384 INFO Loading dictionaries from /Users/Valeriya/anaconda/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-05-19 09:04:59,482 INFO format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def normalize_and_add_pos(word):\n",
    "    p = morph.parse(word)[0]\n",
    "    pos = p.tag.POS\n",
    "    if pos:\n",
    "        if 'ADJ' in pos:\n",
    "            pos = 'ADJ'\n",
    "    else:\n",
    "        pos = ''\n",
    "    return '{}_{}'.format(\n",
    "        p.normal_form,\n",
    "        pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            open_name = os.path.join(root, filename)\n",
    "            if filename in file_hash:\n",
    "                class_text = file_hash[filename]\n",
    "                tree = etree.parse(open_name)\n",
    "                yield tree, class_text, open_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sent = []\n",
    "# for tree, class_text, open_name in read_txt('./corpus'):\n",
    "#     all_p = tree.xpath('//p')\n",
    "#     joined = '\\n'.join([p.text for p in all_p if p.text is not None])\n",
    "#     sent.append((joined, class_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# words = sum([len(text[0].split()) for text in sent])\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Valeriya/Desktop/CoursePaper/corpus/rossija/nesnyat/south/sochi/451403.xhtml\n",
      "/Users/Valeriya/Desktop/CoursePaper/corpus/rossija/nesnyat/south/sochi/451404.xhtml\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for tree, class_text, open_name in read_txt('/Users/Valeriya/Desktop/CoursePaper/corpus'):\n",
    "    body = tree.xpath('//body')\n",
    "    text = '\\n'.join([elem.text for elem in body[0] if elem.text])\n",
    "#     if class_text != 'поздравление':\n",
    "#         class_text = 'другие'\n",
    "    if text:\n",
    "        texts.append((text, class_text, open_name))\n",
    "    else:\n",
    "        print(open_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# words = sum([len(text[0].split()) for text in sent])\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-19 09:05:27,915 INFO loading projection weights from ruwikiruscorpora_0_300_20.bin.gz\n",
      "2017-05-19 09:06:01,918 INFO loaded (392339, 300) matrix from ruwikiruscorpora_0_300_20.bin.gz\n"
     ]
    }
   ],
   "source": [
    "m = 'ruwikiruscorpora_0_300_20.bin.gz'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "# with open('formal_features_3.tsv', 'r') as f:\n",
    "with open('formal_features_3.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.rstrip().split('\\t')\n",
    "        path = items[0]\n",
    "        feats = items[1::]\n",
    "        features[path] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "only_texts = [text[0] for text in texts]\n",
    "classes = [text[1] for text in texts]\n",
    "        \n",
    "data = []\n",
    "for i, (text, class_name, open_name) in enumerate(texts):\n",
    "    text_vectors = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        norm_word = normalize_and_add_pos(word)\n",
    "        if norm_word in model:\n",
    "            text_vectors.append(model[norm_word])\n",
    "    mean_vector = sum(text_vectors) / len(text_vectors)\n",
    "    open_name = open_name.replace('/Users/Valeriya/Desktop/CoursePaper/', './')\n",
    "    formal_features = features[open_name][::]\n",
    "    n_excl = 0\n",
    "    for item in text[0]:\n",
    "        if item == '!':\n",
    "            n_excl+=1\n",
    "\n",
    "    n_ques = 0\n",
    "    for item in text[0]:\n",
    "        if item == '?':\n",
    "            n_ques+=1\n",
    "    formal_features.append(n_excl)\n",
    "    formal_features.append(n_ques)\n",
    "    \n",
    "    formal_features = [float(x) for x in formal_features]\n",
    "    formal_features = formal_features[:2:] + [x/formal_features[0] for x in formal_features[2::]]\n",
    "    \n",
    "    data.append(list(mean_vector)+formal_features)\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(texts_norm)\n",
    "# data['class'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, classes, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2954"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_norm = min_max_scaler.fit_transform(X_train)\n",
    "X_test_norm = min_max_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Классификаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ba86b66bd935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=100)\n",
    "clf1.fit(X_train, y_train)\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "print(classification_report(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6431956668923493"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
